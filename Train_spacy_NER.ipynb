{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_spacy_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1qsYUm9MStEXsnfR_zfovUXtXVE2n01jw",
      "authorship_tag": "ABX9TyMUmqXeVafRqyW9pKyV5+Me",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikucherevsky/Brandefine/blob/main/Train_spacy_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUYXqI3MeeFB"
      },
      "source": [
        "!pip install spacy==2.3.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvQU0UkbKPch",
        "outputId": "c7d84ffd-90fa-457a-b6ee-5cb3158fa49e"
      },
      "source": [
        "import pandas as pd \n",
        "import spacy\n",
        "import pickle\n",
        "spacy.require_gpu()\n",
        "print(\"GPU for spaCy is ready\")\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import re\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "from spacy.gold import GoldParse\n",
        "from spacy.scorer import Scorer\n",
        "from spacy.matcher import PhraseMatcher"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU for spaCy is ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2S3q5DIKivO"
      },
      "source": [
        "!pip install pymorphy2\n",
        "import pymorphy2\n",
        "print('pymorphy2 succesfully installed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z6y5e4ILmsc"
      },
      "source": [
        "#load processed data\n",
        "with open('/content/drive/MyDrive/dataset/TRAINING_DATA.pkl', 'rb') as input:\n",
        "    TRAINING_DATA = pickle.load(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65AetprmjG7A"
      },
      "source": [
        "# Кастомный токенизатор, для токенизация слов, написанных через дефис\n",
        "# как единого токена, в остальном - аналогичен токенизатору по умолчанию\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
        "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
        "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
        "\n",
        "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
        "                                suffix_search=suffix_re.search,\n",
        "                                infix_finditer=infix_re.finditer,\n",
        "                                token_match=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu2EjaeCLyAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19e9ab7-d55d-45fd-a22d-d2da0725ffb2"
      },
      "source": [
        "# multi-model training\n",
        "# Аналогично тренируем вторую модель, на TRAINING_DATA[450000:900000]\n",
        "nlp = spacy.blank('ru')\n",
        "ner = nlp.create_pipe('ner')\n",
        "nlp.add_pipe(ner)\n",
        "ner.add_label('BRAND')\n",
        "ner.add_label('PRODUCT')\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n",
        "\n",
        "BATCH_SIZE = 1000\n",
        "DATA = TRAINING_DATA[:450000]\n",
        "BATCH_SIZE = 1000\n",
        "nlp.begin_training()\n",
        "for it in range(15):\n",
        "    random.shuffle(DATA)\n",
        "    losses = {}\n",
        "    for batch in tqdm(spacy.util.minibatch(DATA, size = BATCH_SIZE)):\n",
        "        texts = [text for text,_ in batch]\n",
        "        annotations = [annotation for _, annotation in batch]\n",
        "        nlp.update(texts, annotations,drop = 0.15, losses = losses)\n",
        "    print(it,losses)\n",
        "nlp.to_disk('NER_WITH_PRODUCT_450000_1st_part')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "450it [05:50,  1.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 {'ner': 332190.5123500824}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:49,  1.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 {'ner': 136115.6136112213}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:52,  1.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2 {'ner': 118800.84375}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:48,  1.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3 {'ner': 111627.76222610474}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:49,  1.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4 {'ner': 106175.42226791382}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:59,  1.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5 {'ner': 101952.70368766785}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:59,  1.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6 {'ner': 99238.39154434204}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:57,  1.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7 {'ner': 97208.0414428711}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:58,  1.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8 {'ner': 96164.64018154144}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:58,  1.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9 {'ner': 94194.99874687195}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [06:04,  1.24it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10 {'ner': 92684.32039451599}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [06:00,  1.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11 {'ner': 91822.36306667328}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:56,  1.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12 {'ner': 90554.33896446228}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [05:54,  1.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13 {'ner': 89670.77554512024}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450it [06:00,  1.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14 {'ner': 89219.70609474182}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiViAn-c2wsb",
        "outputId": "56f1158d-1419-4800-c087-15907a58b2d5"
      },
      "source": [
        "!zip -r ner_45000_1st_part.zip NER_WITH_PRODUCT_450000_1st_part"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NER_WITH_PRODUCT_450000_1st_part/ (stored 0%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/vocab/ (stored 0%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/vocab/vectors (deflated 45%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/vocab/strings.json (deflated 82%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/vocab/key2row (stored 0%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/vocab/lookups.bin (stored 0%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/tokenizer (deflated 74%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/ner/ (stored 0%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/ner/model (deflated 7%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/ner/moves (deflated 56%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/ner/cfg (deflated 47%)\n",
            "  adding: NER_WITH_PRODUCT_450000_1st_part/meta.json (deflated 48%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5WRjOMdfvPF"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KiilqCg1K1A"
      },
      "source": [
        "from spacy.gold import GoldParse\n",
        "from spacy.scorer import Scorer\n",
        "\n",
        "def evaluate(ner_model, examples):\n",
        "    scorer = Scorer()\n",
        "    for input_, annot in examples:\n",
        "        doc_gold_text = ner_model.make_doc(input_)\n",
        "        gold = GoldParse(doc_gold_text, entities=annot['entities'])\n",
        "        pred_value = ner_model(input_)\n",
        "        scorer.score(pred_value, gold)\n",
        "    return scorer.scores   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpUypGSt1VdP",
        "outputId": "feb7b49f-c6f2-498a-8e74-9efda6acb042"
      },
      "source": [
        "evaluate(nlp,TRAINING_DATA[114000:124000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Russian model ru2_combined_400ks_96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ents_f': 95.47040304053095,\n",
              " 'ents_p': 95.44062606328684,\n",
              " 'ents_per_type': {'BRAND': {'f': 92.16297786720322,\n",
              "   'p': 92.06109938699628,\n",
              "   'r': 92.2650820827878},\n",
              "  'PRODUCT': {'f': 99.7463744553554,\n",
              "   'p': 99.81777951321098,\n",
              "   'r': 99.67507148427346}},\n",
              " 'ents_r': 95.50019860409692,\n",
              " 'las': 0.0,\n",
              " 'las_per_type': {'': {'f': 0.0, 'p': 0.0, 'r': 0.0}},\n",
              " 'tags_acc': 0.0,\n",
              " 'textcat_score': 0.0,\n",
              " 'textcats_per_cat': {},\n",
              " 'token_acc': 100.0,\n",
              " 'uas': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNVkarKs1q4N",
        "outputId": "dfc83cda-05f5-455c-97de-ad00eda6da1a"
      },
      "source": [
        "doc = nlp('напиток пивной  жатецкий гусь рубиновый   zatecky gus rubinovy  пастеризованный  бут  самара 0 48')\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "напиток PRODUCT\n",
            "жатецкий гусь BRAND\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}